# ğŸ•·ï¸ Async Distributed Amazon Scraper

An **asynchronous distributed web scraper** for Amazon that leverages:
- ğŸ Python (FastAPI, Scrapy)
- ğŸ‹ Docker
- ğŸ§ª Poetry (dependency management)
- ğŸ§µ Celery (task queue)
- ğŸ§  Redis (broker & result backend)
- â˜ï¸ Boto3 (for pushing scraped data to AWS S3)
- ğŸ“Š ScrapeOps (monitoring)

## ğŸ¥ Demo & Architecture Overview

Watch the full project demo and architecture walkthrough here:  
ğŸ‘‰ ğŸ‘‰ [![Watch on Loom](https://imgur.com/a/zY7D5xz)](https://www.loom.com/share/9816bdf62861418bad64c80378b85e4a?sid=764afa14-4e9e-42e2-8c42-db03a75b011d)




---

## ğŸš€ Features

- ğŸ” **Asynchronous, distributed scraping** powered by Celery
- ğŸ“¦ Fully containerized using Docker
- ğŸ› ï¸ Poetry for modern Python packaging & management
- ğŸ“¡ FastAPI to trigger scrape jobs with keyword and page count
- ğŸ•·ï¸ Scrapy to crawl Amazon product pages
- ğŸ” ScrapeOps integration to manage scraping at scale
- â˜ï¸ Results automatically pushed to **Amazon S3**
- ğŸ“¬ Get job status via Task ID API

---

## âš™ï¸ Tech Stack

| Tool        | Purpose                         |
|-------------|----------------------------------|
| FastAPI     | API server for accepting tasks   |
| Celery      | Distributed task queue           |
| Redis       | Message broker & result backend  |
| Scrapy      | Web scraping framework           |
| Boto3       | Push results to AWS S3           |
| ScrapeOps   | Scraper monitoring & rotation    |
| Poetry      | Python packaging                 |
| Docker      | Containerized deployment         |

---

## ğŸ“¥ How It Works

1. **User sends POST request** to FastAPI with a keyword and number of pages.
2. FastAPI **queues a task** using Celery and Redis.
3. A **Celery worker** picks up the task asynchronously.
4. Worker **runs a Scrapy spider**, scraping Amazon products based on the keyword.
5. Once done, the results are **uploaded to S3** using `boto3`.
6. User can **check task status** using a separate endpoint with the task ID.

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/amazon-distributed-scraper.git
cd amazon-distributed-scraper
poetry install
```

---

## ğŸ³ Run with Docker

```bash
docker compose up --build
```

This spins up:
- Redis
- FastAPI server
- Celery worker

---



---

## ğŸ§  To Do / Roadmap

- [ ] UI dashboard for scraping jobs
- [ ] Retry failed scraping tasks
- [ ] Database to store metadata
